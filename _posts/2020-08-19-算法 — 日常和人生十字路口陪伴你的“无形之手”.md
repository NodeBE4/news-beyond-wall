---
layout: post
title: "算法 — 日常和人生十字路口陪伴你的“无形之手”"
date: 2020-08-19T20:57:27.000Z
author: BBC
from: http://www.bbc.com/zhongwen/simp/science-53820546
tags: [ BBC ]
categories: [ BBC ]
---
<!--1597870647000-->
[算法 — 日常和人生十字路口陪伴你的“无形之手”](http://www.bbc.com/zhongwen/simp/science-53820546)
------

<div>
<figure><img alt="BBC电视剧里的贝多芬" src="https://ichef.bbci.co.uk/news/600/cpsprodpb/15F4A/production/_114003998_b82965d4-9fe1-4d96-8a20-35d93aef0f2e.jpg" referrerpolicy="no-referrer"><br><figcaption>贝多芬：我要扼住命运的咽喉。那个年代没有大数据、算法。假如他活在今天呢？</figcaption></figure><p class="story-body__introduction">大数据、算法、人工智能、互联网、物联网......科技的发展为人类生活带来各种便利的同时，也在静悄悄地决定着人们的生活轨迹和命运。</p><p>假设，如果中国有一年取消高考，用算法根据历年成绩给每个应届毕业生打分，然后大学根据这个分数决定是否录取，录取到什么专业。</p><p>英国今年的中学毕业生对算法的这种威力有切身体验。</p><p>因为新冠疫情而取消了决定数万名中学毕业生命运的考试，用电脑算法( algorithm）给学生打分，让机器根据学校往年的考试结果，经过一番运算，得出今年毕业生的成绩。</p><p>决定命运的是高中毕业生的A-level 考试也决定了能否上大学，能上哪所大学。初中毕业，不再上学而开始求职谋生的，需要凭GCSE考试成绩去找工作。如果准备继续上学，日后申请大学时，这组成绩也是重要参考数据。</p><p>除了这种“十字路口”式的人生关键节点，算法在现代人生活中很多方面扮演着“无形之手”的角色，而我们未必知道。</p><p><strong>BBC科技事务记者克雷顿（James Clayton）和克莱恩曼（Zoe Kleinman）梳理了几个算法左右命运的平台。</strong></p><figure><img alt="Phone with Facebook logo in pocket" src="https://ichef.bbci.co.uk/news/600/cpsprodpb/F54C/production/_113969726_gettyimages-1126502094.jpg" referrerpolicy="no-referrer"><br><figcaption> ©Getty Images</figcaption></figure><h2 class="story-body__crosshead">社交媒体</h2><p>从许多方面来看，社交媒体平台基本上就是庞然大物般的算法。</p><p>它们根据你所提供的，以及来自其他各种渠道的数据，确定你的兴趣、爱好、口味，然后为你推送更多它们认为你喜欢的内容。</p><p>这些数据包括你点的每一个“赞”，看的每一条讯息或视频，点击的每一个链接，都被记录存档。大部分应用软件还会从你的上网习惯和特点，以及地理位置等信息提取更多数据，关于你的数据。这些数据用来判断你喜欢什么，哪些内容吸引你的眼球，能让你在网上流连。</p><p>这个算法对你了解越多，对你生活的影响力就越大。 除了推送更多合你口味的内容，它自然还会用这个知识向你推销产品。</p><p>社交媒体公司收集储存的数据也被用来订制个人化的广告，定向投放，而且精准度高得令人咂舌。</p><p>当然，这类算法也会犯很严重的错误，惹大祸。比如它发现极端的、刺激的、重口味的、充满仇恨和煽动性的内容，在社交媒体上很吃香，比平淡普通的内容更赚眼球、点击和转发更多。所以有些人发现自己收到各种极端主义的、暴力的、仇恨的推送内容。</p><p>脸书针对民权问题做过一次内部审计，审计报告中明确提到，公司必须尽一切所能阻止它的算法把用户“驱赶到极端主义自我强化的回音室里去”。</p><p>算法还有一个盲点，那就是对种族歧视和仇恨类产品不加分辨地推送，导致投诉甚至吃官司。</p><ul class="story-body__unordered-list"><li class="story-body__list-item"><a href="http://www.bbc.com/zhongwen/simp/chinese-news-50328377" class="story-body__link">人脸识别在中港台引发激烈争论的焦点案例</a></li><li class="story-body__list-item"><a href="http://www.bbc.com/zhongwen/simp/science-52542580" class="story-body__link">人工智能帮助诊断新冠肺炎走到哪一步</a></li><li class="story-body__list-item"><a href="http://www.bbc.com/zhongwen/simp/science-43142127" class="story-body__link">爱恨交织 人工智能“双刃剑出鞘”</a></li><li class="story-body__list-item"><a href="http://www.bbc.com/zhongwen/simp/chinese-news-52003034" class="story-body__link">肺炎疫情：面对抗疫科技，中港台在隐私保护与有效性间艰难平衡</a></li></ul><figure><img alt="城镇鸟瞰" src="https://ichef.bbci.co.uk/news/600/cpsprodpb/A7F4/production/_113969924_gettyimages-976183518.jpg" referrerpolicy="no-referrer"><br><figcaption>如果搬家，你的房屋保险或汽车保险保费可能会变，因为不同邮编的风险和“索赔几率”不同 ©Getty Images</figcaption></figure><h2 class="story-body__crosshead">保险</h2><p>保险公司需要评估风险，要做精算，无论是房产险、汽车险、寿险、医疗险，保费的多少依据这个评估结果来定。</p><p>许多人不知道的是，保险业很早就开始探索利用历年积累储存的数据来分析、预测未来；在电脑和算法尚未普及的时候就开始了。</p><p>有了算法，保险公司如虎添翼。</p><p>哈克尼斯（Timandra Harkness）是这方面专家，有一部关于大数据的专著（Big Data: Does Size Matter）。</p><p>有些显而易见但人们通常不太在意的事，背后就有算法的无形之手。</p><p>她举了个例子，比如家庭住址的邮编就决定了你的保费高低，而你在这些问题上毫无发言权，完全被动。</p><p>你的房产或车保险保费在搬家后涨了还是降了，跟你个人没有直接关系，而是跟这个地区的其他居民曾经，或者在多大程度上可能成为犯罪分子的受害者，或者遭遇水淹，或者出车祸，等等。</p><p>现在比较普遍的行车记录仪，汽车上的“黑匣子”，可以用来记录司机的驾车习惯，如果记录显示车主基本上是安全驾驶，即使其他指标都属于高风险群体，他/她的汽车险保费也可能降低。</p><p>这是利用算法制定个人化保险产品的一个例子。</p><p>保险行业的 实质是风险分摊，人人交保费，需要的人从钱库里取。个人化的保险产品能否普及，或许还需要算法来帮忙，确定这类新业务对保险公司有何利弊。</p><ul class="story-body__unordered-list"><li class="story-body__list-item"><a href="http://www.bbc.com/zhongwen/simp/science-51587441" class="story-body__link">谷歌、微软AI和翻译耳机能帮你到什么程度</a></li><li class="story-body__list-item"><a href="http://www.bbc.com/zhongwen/simp/science-42520867" class="story-body__link">在互联网“匿脸不匿名”可行吗？  他做到了</a></li><li class="story-body__list-item"><a href="http://www.bbc.com/zhongwen/simp/world-44859007" class="story-body__link">监控升级：当摄像头能读出你的喜怒哀乐</a></li><li class="story-body__list-item"><a href="http://www.bbc.com/zhongwen/simp/science-45591003" class="story-body__link">AI和媒体 编辑室里写新闻的机器</a></li></ul><figure><img alt="看X光片子" src="https://ichef.bbci.co.uk/news/600/cpsprodpb/0BB4/production/_113969920_gettyimages-838190906.jpg" referrerpolicy="no-referrer"><br><figcaption>看片子诊断病灶，比如乳癌，人工智能比医生肉眼更精准 ©Getty Images</figcaption></figure><h2 class="story-body__crosshead">医疗</h2><p>人工智能（AI）在疾病诊断方面的应用日益普及，有些地方AI甚至可以提出治疗方案和疗法建议。 </p><p>2020年1月一项研究结果显示，通过X光片子诊断乳癌，算法的表现超过医生。</p><p>其他例子也有，比如可以预测子宫癌患者生存率的工具，这个工具还能帮助确定治疗方案。</p><p>伦敦大学学院（UCL）研发的人工智能可以分辨出哪些病人最容易忘记就诊预约，因此需要提醒服务。</p><p>海量数据是训练人工智能的必要前提。这就涉及到许多棘手的问题，包括病人隐私和个人数据保护。</p><p>2017年，英国信息委员会裁定，皇家自由医院NHS信托没有妥善保管病人数据，跟谷歌的人工智能子公司DeepMind分享了160万病人的数据。</p><ul class="story-body__unordered-list"><li class="story-body__list-item"><a href="http://www.bbc.com/ukchina/simp/vert-fut-41415842" class="story-body__link">你所未知的人工智能应用领域</a></li><li class="story-body__list-item"><a href="http://www.bbc.com/ukchina/simp/vert-fut-39875173" class="story-body__link">人工智能受愚弄后也会犯“低级错误”</a></li><li class="story-body__list-item"><a href="http://www.bbc.com/zhongwen/simp/science-48380424" class="story-body__link">人工智能AI的前世今生：神话、科幻和现实</a></li><li class="story-body__list-item"><a href="http://www.bbc.com/zhongwen/simp/science-47361632" class="story-body__link">人工智能：吓坏创造者的“深度造假写手”</a></li></ul><figure><img alt="police car blurred" src="https://ichef.bbci.co.uk/news/600/cpsprodpb/796E/production/_113968013_gettyimages-1227965390.jpg" referrerpolicy="no-referrer"><br><figcaption> ©Getty Images</figcaption></figure><h2 class="story-body__crosshead">警务</h2><p>大数据和机器学习对警察机构来说具有颠覆性的巨大潜力。</p><p>理论上，算法具备了科幻作品中描述的那种“预测警务”（predictive policing）的功能，可以利用历年刑事罪案的数据分析来确定警察资源的部署。</p><p>问题在于这种算法有先天缺陷，容易产生算法偏见，甚至形成算法种族歧视。</p><p>科技智库WebRoots Democracy专家乔杜里（Areeq Chowdhury）解释说，这就跟算法确定考试分数类似，实际上是根据别人过往的学业表现来给你打分，有什么道理？</p><p>他说，根据算法，一个特定社区很可能被不成比例地突出和强调。</p><p>国防和安全智库RUSI今年稍早发表了<a href="https://rusi.org/sites/default/files/rusi_pub_165_2020_01_algorithmic_policing_babuta_final_web_copy.pdf" class="story-body__link-external">一份报告</a>，阐述算法在警务实践中的应用。</p><p>报告提出一些需要关注的问题，包括没有全国统一的准则，也没有影响评估机制。</p><p>另一个需要深入研究的问题是算法如何放大、加剧种族主义倾向。</p><p>警务部门已经在使用的人脸识辨技术受到批评，矛头集中在用于机器学习的数据库是否会导致算法种族主义。</p><p>比如，用于人脸识别的摄像机识别白人更准确，是否因为它们背后的数据库里白人的面部数据更多？</p><p>这方面，数据是否足够多，足够多元化，就至关重要。</p><p>乔杜里说，最应该避免的是因为算法误判而错抓无辜。</p>
</div>
